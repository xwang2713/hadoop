#!/bin/bash

set -e

configure_hosts () {
    private_address=`unit-get private-address`
    # This is a horrible hack to ensure that 
    # Java can resolve the hostname of the server to its
    # real IP address.

    # Fixup stuff in lxc containers
    hostname=`hostname`
    grep -q "^127.0.0.1.*$hostname" /etc/hosts &&
        sed -i -e "s/^\(127.0.0.1 .*\)$hostname/\1/" /etc/hosts &&
        echo "$private_address $hostname" >> /etc/hosts

    # only necessary on oneiric but shouldn't break anything elsewhere
    hostname=`hostname -f`
    sed -i -e "s/^127.0.1.1\(.*$hostname.*\)/$private_address\1/" /etc/hosts
}

configure_sources () {
    source=`config-get source`
    juju-log "Configuring hadoop using the Hadoop Ubuntu Team PPA..."
    add-apt-repository ppa:hadoop-ubuntu/$source
    apt-get update
}

install_base_packages () {
    juju-log "Installing hadoop base..."
    apt-get install -y hadoop dotdee
}

config_element () {
    key=$1
    value=$2
    echo "  <property>"
    echo "    <name>$1</name>"
    echo "    <value>$2</value>"
    echo "  </property>"
}

config_basic () {
    echo "<?xml version=\"1.0\"?>" > $1/01-header
    echo "<configuration>" > $1/02-header
    echo "</configuration>" > $1/99-footer
}

open_ports () {
    case $1 in
        namenode)
            open-port 8020
            open-port 50070 
            ;;
        datanode)
            open-port 50010
            open-port 50020
            open-port 50075
            ;;
        secondarynamenode)
            open-port 50090
            ;;
    esac
}

configure_hadoop () {
    # Copy distribution configuration and then specialize
    if [ ! -d /etc/hadoop/conf.juju ]
    then
        cp -r /etc/hadoop/conf.empty /etc/hadoop/conf.juju
        update-alternatives --install /etc/hadoop/conf hadoop-conf \
            /etc/hadoop/conf.juju 50
        cp /dev/null /etc/hadoop/conf.juju/hdfs-site.xml
        cp /dev/null /etc/hadoop/conf.juju/core-site.xml
        dotdee --setup /etc/hadoop/conf.juju/hdfs-site.xml 
        dotdee --setup /etc/hadoop/conf.juju/core-site.xml 
    fi
    # Configure HDFS
    dir=`dotdee --dir /etc/hadoop/conf.juju/hdfs-site.xml`
    config_basic $dir
    # Purge existing configuration
    rm -f $dir/1*-dfs.*
    config_element "dfs.name.dir" \
        "/var/lib/hadoop/cache/hadoop/dfs/name" > \
        $dir/10-dfs.name.dir
    config_element "dfs.namenode.handler.count" \
        "`config-get dfs.namenode.handler.count`" > \
        $dir/11-dfs.namenode.handler.count
    config_element "dfs.block.size" i\
        "`config-get dfs.block.size`" > \
        $dir/12-dfs.block.size
    config_element "dfs.datanode.max.xcievers" \
        "`config-get dfs.datanode.max.xcievers`" > \
        $dir/13-dfs.datanode.max.xcievers
    [ "`config-get hbase`" = "True" ] && \
        config_element "dfs.support.append" "true" > \
            $dir/14-dfs.support.append || :
    [ "`config-get webhdfs`" = "True" ] && \
        config_element "dfs.webhdfs.enabled" "true" > \
            $dir/15-dfs.webhdfs.enabled || :
    # TODO - secure this hadoop installation!
    config_element "dfs.permissions" "false" > \
            $dir/16-dfs.permissions
    dotdee --update /etc/hadoop/conf.juju/hdfs-site.xml || true
    # Configure Hadoop Core
    dir=`dotdee --dir /etc/hadoop/conf.juju/core-site.xml`
    config_basic $dir
    rm -f $dir/1*-*
    config_element "hadoop.tmp.dir" "/var/lib/hadoop/cache/\${user.name}" > \
        $dir/10-hadoop.tmp.dir
    config_element "io.file.buffer.size" "`config-get io.file.buffer.size`" > \
        $dir/11-io.file.buffer.size
    dotdee --update /etc/hadoop/conf.juju/core-site.xml || true
}

configure_role () {
    dir=`dotdee --dir /etc/hadoop/conf.juju/core-site.xml`
    juju-log "Configuring service unit as $1..."
    case $1 in
        datanode|secondarynamenode)
            namenode_address=`relation-get private-address`
            config_element "fs.default.name" "hdfs://$namenode_address:8020" > \
                $dir/50-fs.default.name
            ;;
        namenode)
            private_address=`unit-get private-address`
            config_element "fs.default.name" "hdfs://$private_address:8020" > \
                $dir/50-fs.default.name
            ;;
    esac
    dotdee --update /etc/hadoop/conf.juju/core-site.xml || true
    dir=`dotdee --dir /etc/hadoop/conf.juju/hdfs-site.xml`
    case $1 in
        secondarynamenode)
            namenode_address=`relation-get private-address`
            config_element "dfs.http.address" "$namenode_address:50070" > \
                $dir/50-dfs.http.address
            ;;
    esac
    dotdee --update /etc/hadoop/conf.juju/hdfs-site.xml || true
}

install_packages () {
    case $1 in
        namenode|datanode|secondarynamenode) 
            juju-log "Installing extra packages for $1"
            apt-get -y install hadoop-$1
            ;;
        *)
            juju-log "Unsupported role $1..."
            ;;
    esac
}

format_namenode () {
    juju-log "Formatting namenode filesystem"
    su hdfs -c "hadoop namenode -format"
}

# TODO - Add these to charm-tools helpers.
_restart_ () {
    juju-log "Restarting $1"
    service $1 restart || :
}
_start_ () {
    juju-log "Starting $1"
    service $1 start || :
}
_stop_ () {
    juju-log "Stopping $1"
    service $1 stop || :
}

# Hadoop Service Control Commands
restart_hadoop () { _restart_ hadoop-$1; }
stop_hadoop () { _stop_ hadoop-$1; }

# Determines what type of node this is
resolve_role () {
    role="unconfigured"
    [ -d /usr/share/doc/hadoop-namenode ] && \
        role="namenode" || :
    [ -d /usr/share/doc/hadoop-secondarynamenode ] && \
        role="secondarynamenode" || :
    [ -d /usr/share/doc/hadoop-datanode ] && \
        role="datanode" || :
    echo "$role"
}
role=`resolve_role`

COMMAND=`basename $0`

case $COMMAND in
    install)
        configure_hosts
        configure_sources
        install_base_packages
        configure_hadoop
        ;;
    start|stop)
        # No-op
        ;;
    namenode-relation-joined)
        case $role in
            unconfigured)             
                juju-log "Configuring this unit as a namenode"
                role="namenode"
                install_packages $role
                configure_role $role
                stop_hadoop $role
                format_namenode
                restart_hadoop $role
                open_ports $role
                ;;
            namenode)
                juju-log "Already configured as namenode"
                # Unit should only ever assume this role once so no
                # further action is required - this prevents adding
                # an additional unit to the namenode master breaking
                # things.
                ;;
            *)
                juju-log "Already configured as another role: $role"
                exit 1
                ;;
        esac
        ;;
    secondarynamenode-relation-joined)
        case $role in
            unconfigured)
                juju-log "Configuring this unit as a secondarynamenode"
                role="secondarynamenode"
                install_packages $role
                configure_role $role
                restart_hadoop $role
                open_ports $role
                ;;
            secondarynamenode)
                juju-log "Already configured as secondarynamenode"
                # Unit should only ever assume this role once so no
                # further action is required - this prevents adding
                # an additional unit to the namenode master breaking
                # things.
                ;;
            *)
                juju-log "Already configured as another role: $role"
                exit 1
                ;;
        esac
        ;;
    datanode-relation-joined)
        case $role in
            unconfigured) 
                juju-log "Configuring this unit as a datanode"
                role="datanode"
                install_packages $role
                configure_role $role
                restart_hadoop $role
                open_ports $role
                ;;
            datanode)
                juju-log "Already configured as datanode"
                # Unit should only ever assume this role once so no
                # further action is required - this prevents adding
                # an additional unit to the namenode master breaking
                # things.
                ;;
            *)
                juju-log "Already configured as another role: $role"
                exit 1
                ;;
        esac
        ;;
    upgrade-charm)
        configure_hadoop
        restart_hadoop $role
        open_ports $role
        ;;
    *)
        juju-log "Command not recognised"
        ;;
esac
